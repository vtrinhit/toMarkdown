services:
  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
    container_name: tomd-backend
    restart: unless-stopped
    ports:
      - "8000:8000"
    environment:
      - DEBUG=false
      - MAX_WORKERS=4
      - MAX_UPLOAD_SIZE=104857600
      - OPENAI_API_KEY=${OPENAI_API_KEY:-}
      - OPENAI_BASE_URL=${OPENAI_BASE_URL:-}
      - PYTHONUNBUFFERED=1
    volumes:
      - tomd_uploads:/tmp/tomd_uploads
      - tomd_outputs:/tmp/tomd_outputs
      # For ML model caching
      - tomd_cache:/root/.cache
    deploy:
      resources:
        limits:
          memory: 8G
        reservations:
          memory: 2G
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
    container_name: tomd-frontend
    restart: unless-stopped
    ports:
      - "80:80"
    depends_on:
      backend:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:80"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s

volumes:
  tomd_uploads:
  tomd_outputs:
  tomd_cache:

# For GPU support (NVIDIA), uncomment below:
# services:
#   backend:
#     deploy:
#       resources:
#         reservations:
#           devices:
#             - driver: nvidia
#               count: 1
#               capabilities: [gpu]
